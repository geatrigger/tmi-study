# Hadoop Ecosystem이란

* https://inkkim.github.io/hadoop/Hadoop-Ecosystem%EC%9D%B4%EB%9E%80/

# Hadoop 2와 3의 차이

* https://blog.geunho.dev/posts/hadoop-docker-test-env-hdfs/

# Hadoop 설치

* https://www.guru99.com/how-to-install-hadoop.html

* https://hadoop.apache.org/docs/r3.3.1/hadoop-project-dist/hadoop-common/SingleCluster.html

* https://blog.geunho.dev/posts/hadoop-docker-test-env-hdfs/

* 하둡 클러스터 도커를 이용해 설치

  * https://eyeballs.tistory.com/144

* 도커를 이용해 dependencies를 간편히 설치

  * 버전 다른 거 설치
  * https://github.com/apache/hadoop/blob/rel/release-3.2.1/dev-support/docker/Dockerfile

* git에서 발생한 문제

  * https://github.com/apache/hadoop/tree/rel/release-3.2.1

  * 해당 프로젝트를 윈도우에서 클론할 때 Filename too long 에러 발생

    ![image-20210708144658559](210714Hadoop.assets/image-20210708144658559.png)

  * 윈도우 API의 파일경로길이 제한이 260자라서 발생한 문제
  * `git config --system core.longpaths true` 로 git 설정을 바꿔서 해결

* 도커로 의존성 설치 과정

  * 2400초(40분) 걸림

  ![image-20210708155005588](210714Hadoop.assets/image-20210708155005588.png)

* 하둡파일 추가 후 다시 빌드

  * 캐시된 기록 덕에 빌드 시간 단축

    ![image-20210708164948112](210714Hadoop.assets/image-20210708164948112.png)

* 필요했던 작업들

  * HADOOP_HOME, JAVA_HOME 환경변수 지정

  * `$HADOOP_HOME/etc/hadoop/hadoop-env.sh`에 필요한 환경변수 지정

    ```shell
    export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
    export HDFS_NAMENODE_USER=root
    export HDFS_DATANODE_USER=root
    export HDFS_SECONDARYNAMENODE_USER=root
    export YARN_RESOURCEMANAGER_USER=root
    export YARN_NODEMANAGER_USER=root
    ```

    

  * ssh  설치 및 localhost접속 가능하도록 세팅

    * https://stackoverflow.com/questions/24319662/from-inside-of-a-docker-container-how-do-i-connect-to-the-localhost-of-the-mach

    * docker는 기본적으로 network값이 bridge로 docker0라는 이름의 bridge를 만든다

    * .network값을 host로 바꾸면 docker container안에서 localhost는 docker host를 지칭하게 된다

    * `docker run -p 22:22 --network="host" --name=hadoop -it geatrigger/hadoop`

    * ssh

      ```shell
      ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
      cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
      chmod 0600 ~/.ssh/authorized_keys
      # ssh 서비스 시작
      /etc/init.d/ssh start
      ```
    * 하둡 single node cluster실행, 중단

      ```shell
      $HADOOP_HOME/sbin/start-dfs.sh
      $HADOOP_HOME/sbin/start-yarn.sh
      # verify whether all the Hadoop related processes are running or not.
      jps
      $HADOOP_HOME/sbin/stop-dfs.sh
      $HADOOP_HOME/sbin/stop-yarn.sh
      ```


​      