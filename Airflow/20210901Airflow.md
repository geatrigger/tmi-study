## Airflow

빅데이터 분석이나 머신러닝을 하다 보면 여러 task들을 연결해서 수행해야 하는 경우가 많다. 데이터베이스에서의 ETL 작업과 비슷한 흐름이라 보면 된다.

예를 들어 머신러닝의 학습 과정을 보면 데이터 전처리, 학습, 예측과 같은 단계를 가지게 된다.

![ml](https://t1.daumcdn.net/cfile/tistory/266DB23B596A16242E)

- rawdata를 읽어서 preprocessing 단계를 거쳐서 학습에 적절한 training data로 변경하고,
- 변경된 training data를 가지고 머신러닝 모델을 학습한후, 학습된 모델을 저장한다.
- 학습된 모델을 가지고 예측을 해서 결과를 저장한다.

머신러닝은 위와 같이 여러 단계를 거쳐 수행이 되는데 순차적으로 수행이 되어야 한다. 단순히 차례대로 실행해도 되지만, 에러가 났을때 재처리를 하거나, 수행 결과에 따라 분기를 하는 등 좀 더 구조화된 도구가 필요하다.



이러한 요구 사항들 때문에 여러 툴이 개발되었는데 대표적인 data workflow 도구로는 HadoopEcosystem Oozie가 있다. 이 외에도 rundeck, luigi와 같은 유사한 솔루션들도 존재한다.

Apache Airflow는 Python 기반으로 task 코드를 작성할 수 있기에 데이터 분석가들에게 접근성이 좋으며, 한대에서 동작하는게 아니라 여러 머신에 분산하여 수행할 수 있는 장점이 있다.

![air](https://t1.daumcdn.net/cfile/tistory/273F0833596A162403)



Airflow를 통해서 데이터엔지니어링의 ETL 작업을 자동화하고, DAG(Directed Acyclic Graph) 형태의 워크플로우 작성이 가능하다.

- Scheduler : 모든 DAG와 Task에 대하여 모니터링 및 관리하고, 실행해야할 Task를 스케줄링 해준다

- Web server : Airflow의 웹 UI 서버다

- DAG : Directed Acyclic Graph로 개발자가 Python으로 작성한 워크플로우를 말한다. Task들의 dependency를 정의한다

- Database : Airflow에 존재하는 DAG와 Task들의 메타데이터를 저장하는 데이터베이스이다

- Worker : 실제 Task를 실행하는 주체입니다. Executor 종류에 따라 동작 방식이 다양하다

  

Airflow는 개발자가 작성한 Python DAG를 읽고, 거기에 맞춰 Scheduler가 Task를 스케줄링하면, Worker가 Task를 가져가 실행한다. Task의 실행상태는 Database에 저장되고, 사용자는 UI를 통해서 각 Task의 실행 상태, 성공 여부 등을 확인할 수 있다.

  

#### 설치 및 실습

- 설치

  ```bash
  sudo pip install apache-airflow
  ```

- 설치 후, 한번만 해주면 되는 작업으로 cfg(환경설정파일)과 기본 데이터베이스(SQLite) 초기화

  ```bash
  airflow db init
  ```

- airflow 버전 확인

  ```bash
  airflow version
  ```

- 등록된 dag 목록 조회

  ```bash
  airflow dags list
  ```

- 사용자 환경변수에 airflow 경로 추가 및 활성화

  ```bash
  echo 'export AIRFLOW_HOME=~/airflow' >> /home/statice/.profile
  echo 'export AIRFLOW_HOME=~/airflow' >> /home/statice/.bashrc
  source ~/.profile
  ```

- dag 파일을 저장할 디렉토리 생성

  ```bash
  mkdir $AIRFLOW_HOME/dags
  ```

- dag파일 생성

  ```bash
  cd $AIRFLOW_HOME/dags/ && vim my_python_op.py
  ```

- 소스 코드 입력 (PythonOperator 인 경우)

  ```bash
  from airflow.models import DAG
  from airflow.utils.dates import days_ago
  from airflow.operators.python_operator import PythonOperator
  import time
  from pprint import pprint
  
  args = {'owner': 'jovyan',
          'start_date': days_ago(n=1)}
  
  dag = DAG(dag_id='my_python_dag',
            default_args=args,
            schedule_interval='@daily')
  
  
  def print_fruit(fruit_name, **kwargs):
      print('=' * 60)
      print('fruit_name:', fruit_name)
      print('=' * 60)
      pprint(kwargs)
      print('=' * 60)
      return 'print complete!!!'
  
  def sleep_seconds(seconds, **kwargs):
      print('=' * 60)
      print('seconds:' + str(seconds))
      print('=' * 60)
      pprint(kwargs)
      print('=' * 60)
      print('sleeping...')
      time.sleep(seconds)
      return 'sleep well!!!'
  
  t1 = PythonOperator(task_id='task_1',
                      provide_context=True,
                      python_callable=print_fruit,
                      op_kwargs={'fruit_name': 'apple'},
                      dag=dag)
  
  t2 = PythonOperator(task_id='task_2',
                      provide_context=True,
                      python_callable=print_fruit,
                      op_kwargs={'fruit_name': 'banana'},
                      dag=dag)
  
  t3 = PythonOperator(task_id='task_3',
                      provide_context=True,
                      python_callable=sleep_seconds,
                      op_kwargs={'seconds': 10},
                      dag=dag)
  
  t4 = PythonOperator(task_id='task_4',
                      provide_context=True,
                      python_callable=print_fruit,
                      op_kwargs={'fruit_name': 'cherry'},
                      dag=dag)
  
  t1 >> [t2, t3]
  [t2, t3] >> t4
  ```

- 코드 실행

  ```bash
  cd $AIRFLOW_HOME/dags/ && python my_python_op.py
  ```

- airflow 스케줄려 실행

  ```bash
  airflow scheduler
  ```

- 스케줄러 컨트롤, 관리할 웹서버 실행

  ```bash
  # start the web server, default port is 8080 
  airflow webserver --port 8080
  ```

  http://localhost:8080/

- off -> on 으로 활성화

  ![web](https://postfiles.pstatic.net/MjAxOTA2MThfMjY4/MDAxNTYwODY1MjMwOTQ5.0KWLzamfr5MDJdixhPSf2kLdlKTuWKZygbMcPEZRoWkg.drSsGqgbeMdYaNleJ9EopdwAjZ3gOqPyjVpj16q8xwIg.JPEG.wideeyed/41.jpg?type=w773)

- Graph View, Tree View, Gantt 탭에서 지정한 task순서 확인가능

  ![task](https://postfiles.pstatic.net/MjAxOTA2MThfMjQ5/MDAxNTYwODY1NjMwMjE0.hLAS6Z3rvPWAKYLS3cPx7hSGup5qc4-Nm-rggSL4f5cg.-uVu-Fh7v_vZIuTdXR2lnqMN0ohcK1Scq9niH1_MlGgg.JPEG.wideeyed/51.jpg?type=w773)

- Task Duration에서 각 task의 수행시간을 그래프로 확인 가능

  ![dura](https://t1.daumcdn.net/cfile/tistory/22452B37596A16251A)

  어떤 task가 시간이 많이 걸리는지 매번 수행할때마다 올바른지(큰 변화가 없고 일정한지) 확인가능



이미 링크드인의 Azkaban이나 스포티파이의  Luigi, 하둡의 Oozie 등 여러 WorkFlow 관리 시스템이 있지만, 나온지 얼마 되지도 않은 Airflow를 주목하는 이유는 분산 환경 지원이 가능하고,  task에 대한 스크립트를 Python을 사용하여 작성할 수 있기 때문에, 각종 빅데이터 분석 시스템이나 라이브러리, 머신러닝 시스템과 연동이 쉽고 Python언어만 알면 쉽게 정교한 플로우 개발이 가능하기 때문에 활용성이 아주 높기 때문이다.



airflow를 선택한 사례

https://tech.socarcorp.kr/data/2021/03/24/what-socar-data-engineering-team-does.html

https://www.bucketplace.co.kr/post/2021-04-13-%EB%B2%84%ED%82%B7%ED%94%8C%EB%A0%88%EC%9D%B4%EC%8A%A4-airflow-%EB%8F%84%EC%9E%85%EA%B8%B0/







