# Spark

* rdd, dataframe, dataset란

  * https://timewizhan.tistory.com/entry/Spark-RDD-vs-Dataframes-vs-Datasets

  * rdd란

    * 여러 노드에 분산저장되어 있는(분산), 변경할 수 없는(불변성), 장애 내성을 보장하는 데이터의 집합이다

  * rdd를 사용하는 경우

    * low-level API인 Transformation, Action을 사용할 때
    * 데이터가 미디어와 같이 비구조화 형태로 되어 있을 때
    * 데이터를 함수형 프로그래밍으로 조작하고 싶을 때
    * 데이터를 처리할 때, 스키마를 굳이 따지고 싶지 않을 때

  * dataframe이란

    * RDD의 성질 포함

    * 관계형 테이블처럼 컬럼이 존재하는 변경할 수 없는 데이터 집합이다
    * Spark 2.0에서는 Dataset의 일종으로 표현한다(Dataset[Row])

  * dataframe을 사용하는 경우

    * SQL(SparkSQL)을 사용하여 데이터를 접근하고 싶을 때
    * Catalyst Optimizer(책 220쪽)

  * pandas dataframe과 spark dataframe의 차이

    * spark dataframe의 경우 분산 처리를 위해 설계되어 있고 카탈리스트 엔진을 통해 리소스 사용량을 실시간으로 최적화할 수 있다

  * dataset이란

    * Dataframe의 성질 포함
    * Type-safe
    * 사용자에게 도메인 객체에 대한 변환 연산을 손쉽게 표현할 수 있는 API를 지원하고, 스파크 SQL 실행 엔진의 빠른 성능과 높은 안정성 제공이 목표
    * 일반 자바 객체를 Dataset에 저장 가능

  * dataset을 사용하는 경우

    * Query Plan이 필요할 때

* spark에서 장애 복구 매커니즘 설명

  * RDD는 데이터셋을 만드는데 사용된 변환 연산자의 로그를 남긴다
  * 노드에 장애가 발생하면 해당 노드가 가진 데이터셋만 로그를 이용해 다시 계산해 RDD를 복원한다

* application submit 과정

  * https://spark.apache.org/docs/latest/

  * https://spark.apache.org/docs/latest/cluster-overview.html

  * driver program 안에 SparkContext 오브젝트가 있다

  * SparkContext는 cluster manager(Spark standalone cluster manager, Mesos, Yarn, Kubernetes 등)과 연결하고 자원을 요청하여 Worker Node의 Executor를 생성한다

  * Spark Context는 Executor들에게 application code(JAR또는 Python file)를 전달하고 executor에서 task를 실행한다

  * 각 application은 각 executor들을 가지고 있어 application끼리 독립적으로 실행이 가능하지만, external storage를 제외하곤 data를 공유할 방법이 없다

    ![Spark cluster components](211110Spark.assets/cluster-overview.png)

* map reduce와의 차이

  * https://www.ibm.com/cloud/blog/hadoop-vs-spark
  * spark는 memory를 사용하고 map reduce는 중간에 disk를 사용하기 때문에 spark가 대체로 3배, 최대 100배 빠르다
  * spark는 real-time processing, unstructured data stream처리에 적합하고 map reduce는 batch processing, linear data processing에 적합하다
  * 데이터가 매우 커지면, map reduce와 spark 모두 HDFS에 의지한다
  * spark에는 ML 라이브러리가 있어 다양한 ML을 돌릴 수 있다

* 분산형 공유변수

  * https://spark.apache.org/docs/latest/rdd-programming-guide.html#shared-variables
  * Broadcast variable
    * 공유변수
    * 여러 클러스터 노드가 공유하는 변수로 읽기 연산만 가능
    * 대규모 데이터를 공유할 때 사용
  * Accumulator
    * 누적변수
    * 여러 클러스터 노드가 공유하는 변수로 값을 더하는 연산만 허용
    * 전역 합계나 카운터 구현할 때 사용



* Transformation과 Action의 차이
  * https://twowinsh87.github.io/data/2018/08/02/data-fcdes-spark-3-persistNCaching/
  * Transformation
    * 입력데이터로 새로운 데이터셋(RDD)을 만들어내는 것
    * action을 하기 전까지는 실행되지 않는다
    * map, filter
  * Action
    * 데이터셋으로부터 RDD와는 다른 타입 값으로 계산하는 것
    * action으로 실제 연산이 수행된다
    * reduce, count